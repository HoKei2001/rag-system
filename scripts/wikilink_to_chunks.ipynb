{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import ast\n",
    "\n",
    "ds = load_dataset(\"google/frames-benchmark\")\n",
    "\n",
    "question=ds['test'][19]['Prompt']\n",
    "answer=ds['test'][19]['Answer']\n",
    "references_doc=ast.literal_eval(ds['test'][19]['wiki_links'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    page_content: str\n",
    "    metadata: dict = field(default_factory=dict)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Initialize metadata title and url if not present\n",
    "        self.metadata.setdefault('title', '')\n",
    "        self.metadata.setdefault('url', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/United_States_federal_executive_departments#Former_departments\n",
      "United States federal executive departments\n",
      "Saved pages to /doc/United States federal executive departments#Former departments.json\n",
      "https://en.wikipedia.org/wiki/United_States_Secretary_of_Homeland_Security\n",
      "United States Secretary of Homeland Security\n",
      "Saved pages to /doc/United States Secretary of Homeland Security.json\n",
      "https://en.wikipedia.org/wiki/Tom_Ridge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/hokei/.cache/pypoetry/virtualenvs/app-WS1thcCm-py3.10/lib/python3.10/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /opt/hokei/.cache/pypoetry/virtualenvs/app-WS1thcCm-py3.10/lib/python3.10/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tom Ridge\n",
      "Saved pages to /doc/Tom Ridge.json\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from urllib.parse import unquote\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def get_wikipedia_title(url):\n",
    "    # Split the URL to get the last part after \"/wiki/\"\n",
    "    title_part = url.split('/wiki/')[-1]\n",
    "    # Decode any percent-encoded characters, e.g., spaces represented as %20\n",
    "    title = unquote(title_part)\n",
    "    # Replace underscores with spaces if needed\n",
    "    title = title.replace('_', ' ')\n",
    "    return title\n",
    "\n",
    "\n",
    "def link_to_json_file(wiki_link: str, language: str):\n",
    "    wiki_query = get_wikipedia_title(wiki_link)\n",
    "    try:\n",
    "        pages = WikipediaLoader(query=wiki_query.strip(), lang=language, load_all_available_meta=False).load()\n",
    "        file_name = wiki_query.strip()\n",
    "        return file_name, pages\n",
    "    except Exception as e:\n",
    "        job_status = \"Failed\"\n",
    "        message = \"Failed To Process Wikipedia Query\"\n",
    "        error_message = str(e)\n",
    "        file_name = wiki_query.strip()\n",
    "        return file_name, {\n",
    "            \"job_status\": job_status,\n",
    "            \"message\": message,\n",
    "            \"error\": error_message,\n",
    "            \"file_name\": file_name\n",
    "        }\n",
    "\n",
    "\n",
    "for i in range(0, len(references_doc)):\n",
    "    print(references_doc[i])\n",
    "    file_name, pages = link_to_json_file(references_doc[i], 'en')\n",
    "    file_name = file_name.replace(\" \", \"_\")\n",
    "    combined_content = \"\\n\".join([page.page_content for page in pages])\n",
    "    title=pages[0].metadata['title']\n",
    "    print(title)\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs('./doc/', exist_ok=True)\n",
    "\n",
    "    # Save the pages to a file\n",
    "    with open(f'./doc/{file_name}.json', 'w') as f:\n",
    "        content=Document(\n",
    "            page_content=combined_content,\n",
    "            metadata= {'title':pages[0].metadata['title'],'url':references_doc[0]}\n",
    "        )\n",
    "        json.dump(content, f)\n",
    "\n",
    "    print(f\"Saved pages to /doc/{file_name}.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def split2chunks(text: str,chunk_size:int,chunk_overlap:int) -> List[str]:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def generate_md5_hash(input_string):\n",
    "    # Create an MD5 hash object\n",
    "    md5_hash = hashlib.md5()\n",
    "    \n",
    "    # Update the hash object with the bytes of the input string\n",
    "    md5_hash.update(input_string.encode('utf-8'))\n",
    "    \n",
    "    # Get the hexadecimal representation of the hash\n",
    "    return md5_hash.hexdigest()\n",
    "\n",
    "def docs_to_chunks_json(doc_data:Document,chunk_size:int,chunk_overlap:int):\n",
    "    doc_chunks={\n",
    "        \"doc_id\":doc_data.metadata['title'].replace(\" \",\"_\"),\n",
    "        \"original_uuid\": generate_md5_hash(doc_data.metadata['title'].replace(\" \",\"_\")),\n",
    "        \"content\":doc_data.page_content\n",
    "    }\n",
    "    chunk_list=split2chunks(doc_data.page_content,chunk_size,chunk_overlap)\n",
    "    chunks=[]\n",
    "    for i in range(len(chunk_list)):\n",
    "        chunk_obj={\n",
    "            \"chunk_id\":doc_chunks['doc_id']+\"_chunk_\"+str(i),\n",
    "            \"original_index\": i,\n",
    "            \"content\":chunk_list[i]\n",
    "        }\n",
    "        chunks.append(chunk_obj)\n",
    "    doc_chunks['chunks']=chunks\n",
    "    return doc_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "# Traverse files under the 'doc/' directory\n",
    "for file_name in os.listdir('doc/'):\n",
    "    if file_name.endswith('.json'):\n",
    "        file_path = os.path.join('doc/', file_name)\n",
    "        codebase_chunks=[]\n",
    "        with open(file_path) as file:\n",
    "            doc_data = json.load(file)\n",
    "            document = Document(page_content=doc_data['page_content'], metadata=doc_data['metadata'])\n",
    "            chunked_data = docs_to_chunks_json(document, chunk_size=1000, chunk_overlap=100)\n",
    "            codebase_chunks.append(chunked_data)\n",
    "            # Ensure the directory exists\n",
    "            os.makedirs('./chunked/', exist_ok=True)\n",
    "\n",
    "            # Save the chunked data to a file\n",
    "            with open(f\"./chunked/chunks_{file_name}\", 'w') as f:\n",
    "                json.dump(codebase_chunks, f,indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-WS1thcCm-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
